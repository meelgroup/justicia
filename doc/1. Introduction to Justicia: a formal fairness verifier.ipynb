{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Justicia : formal fairness verifier\n",
    "This goal of this tutorial is to illustrate how to formally verify different fairness metrics of a prediction algorithm in presence of compound sensitive groups. In this tutorial, the inputs are \n",
    "- a prediction algorithm represented as a CNF (Conjunctive Normal Form) formula,\n",
    "- the probability distribution of input features (computed from finite sampled dataset).\n",
    "\n",
    "Fairness literature has developed multiple definitions of fairness, of which we verify (1) disparate impact and (2) statistical parity definitions. For an elaborate discussion, we refer to [[1]](https://arxiv.org/pdf/2009.06516.pdf).\n",
    "\n",
    "\n",
    "In the following, we show three different prediction algorithms (classifiers) and verify fairness on each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. CNF classifier\n",
    "\n",
    "At first, we consider a CNF classifier that can learn a CNF formula in a dataset with binary class labels. In this tutorial, we use the CNF learner from [[2]](https://bishwamittra.github.io/publication/imli-ghosh.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyrulelearn.imli import imli\n",
    "import sys\n",
    "\n",
    "# From this framework\n",
    "import justicia.utils\n",
    "from justicia.metrics import Metric\n",
    "\n",
    "# data\n",
    "sys.path.append(\"..\")\n",
    "from data.objects.adult import Adult\n",
    "from data.objects.titanic import Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitive attributes: ['race', 'age']\n",
      "-number of samples: (before dropping nan rows) 32561\n",
      "-number of samples: (after dropping nan rows) 32561\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age_0</th>\n",
       "      <th>age_1</th>\n",
       "      <th>age_2</th>\n",
       "      <th>age_3</th>\n",
       "      <th>race_Amer-Indian-Eskimo</th>\n",
       "      <th>race_Asian-Pac-Islander</th>\n",
       "      <th>race_Black</th>\n",
       "      <th>race_Other</th>\n",
       "      <th>race_White</th>\n",
       "      <th>...</th>\n",
       "      <th>capital-gain_1</th>\n",
       "      <th>capital-gain_3</th>\n",
       "      <th>capital-loss_0</th>\n",
       "      <th>capital-loss_1</th>\n",
       "      <th>capital-loss_2</th>\n",
       "      <th>capital-loss_3</th>\n",
       "      <th>hours-per-week_0</th>\n",
       "      <th>hours-per-week_1</th>\n",
       "      <th>hours-per-week_2</th>\n",
       "      <th>hours-per-week_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sex  age_0  age_1  age_2  age_3  race_Amer-Indian-Eskimo  \\\n",
       "0        0      0      1      0      0                        0   \n",
       "1        0      0      1      0      0                        0   \n",
       "2        0      0      1      0      0                        0   \n",
       "3        0      0      1      0      0                        0   \n",
       "4        1      1      0      0      0                        0   \n",
       "...    ...    ...    ...    ...    ...                      ...   \n",
       "32556    1      1      0      0      0                        0   \n",
       "32557    0      0      1      0      0                        0   \n",
       "32558    1      0      0      1      0                        0   \n",
       "32559    0      1      0      0      0                        0   \n",
       "32560    1      0      1      0      0                        0   \n",
       "\n",
       "       race_Asian-Pac-Islander  race_Black  race_Other  race_White  ...  \\\n",
       "0                            0           0           0           1  ...   \n",
       "1                            0           0           0           1  ...   \n",
       "2                            0           0           0           1  ...   \n",
       "3                            0           1           0           0  ...   \n",
       "4                            0           1           0           0  ...   \n",
       "...                        ...         ...         ...         ...  ...   \n",
       "32556                        0           0           0           1  ...   \n",
       "32557                        0           0           0           1  ...   \n",
       "32558                        0           0           0           1  ...   \n",
       "32559                        0           0           0           1  ...   \n",
       "32560                        0           0           0           1  ...   \n",
       "\n",
       "       capital-gain_1  capital-gain_3  capital-loss_0  capital-loss_1  \\\n",
       "0                   0               0               1               0   \n",
       "1                   0               0               1               0   \n",
       "2                   0               0               1               0   \n",
       "3                   0               0               1               0   \n",
       "4                   0               0               1               0   \n",
       "...               ...             ...             ...             ...   \n",
       "32556               0               0               1               0   \n",
       "32557               0               0               1               0   \n",
       "32558               0               0               1               0   \n",
       "32559               0               0               1               0   \n",
       "32560               0               0               1               0   \n",
       "\n",
       "       capital-loss_2  capital-loss_3  hours-per-week_0  hours-per-week_1  \\\n",
       "0                   0               0                 0                 1   \n",
       "1                   0               0                 1                 0   \n",
       "2                   0               0                 0                 1   \n",
       "3                   0               0                 0                 1   \n",
       "4                   0               0                 0                 1   \n",
       "...               ...             ...               ...               ...   \n",
       "32556               0               0                 0                 1   \n",
       "32557               0               0                 0                 1   \n",
       "32558               0               0                 0                 1   \n",
       "32559               0               0                 1                 0   \n",
       "32560               0               0                 0                 1   \n",
       "\n",
       "       hours-per-week_2  hours-per-week_3  \n",
       "0                     0                 0  \n",
       "1                     0                 0  \n",
       "2                     0                 0  \n",
       "3                     0                 0  \n",
       "4                     0                 0  \n",
       "...                 ...               ...  \n",
       "32556                 0                 0  \n",
       "32557                 0                 0  \n",
       "32558                 0                 0  \n",
       "32559                 0                 0  \n",
       "32560                 0                 0  \n",
       "\n",
       "[32561 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbose = True\n",
    "dataset = Adult(verbose=verbose, config=5)\n",
    "df = dataset.get_df()\n",
    "# discretize\n",
    "df =  justicia.utils.get_discretized_df(df, columns_to_discretize=dataset.continuous_attributes, verbose=False)\n",
    "# get X,y\n",
    "X = df.drop(['target'], axis=1)\n",
    "y = df['target']\n",
    "# one-hot\n",
    "X = justicia.utils.get_one_hot_encoded_df(X,X.columns.to_list(), verbose=False)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle = True, random_state=2) # 70% training and 30% test\n",
    "\n",
    "clf = imli(num_clause=2, data_fidelity=10, work_dir=\"../data/\", rule_type=\"CNF\", verbose=False)\n",
    "clf.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned rule:\n",
      "capital-gain_1 OR capital-gain_3 OR capital-loss_1 OR capital-loss_2 AND\n",
      "education-num_3 OR hours-per-week_2 OR not capital-gain_0\n",
      "\n",
      "Train Accuracy: 0.7757302757302758\n",
      "Test Accuracy: 0.7688056493705864\n"
     ]
    }
   ],
   "source": [
    "print(\"learned rule:\")\n",
    "print(clf.get_rule(X_train.columns.to_list()))\n",
    "print(\"\\nTrain Accuracy:\", sklearn.metrics.accuracy_score(clf.predict(X_train.values),y_train.values))\n",
    "print(\"Test Accuracy:\", sklearn.metrics.accuracy_score(clf.predict(X_test.values),y_test.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formally measure different fairness metrics\n",
    "\n",
    "Justicia reduces the verification problem to solving appropriately designed SSAT (Stochastic Satisfiability) instances. Two different encodings have been proposed in \\[1\\]\n",
    "\n",
    "- enumeration (Enum) based encoding and\n",
    "- learning (Learn) based encoding.\n",
    "\n",
    "In the Enum encoding, we formally measure fairness metrics such as disparate impact and statistical parity on each combination of compound sensitive groups. Consider sensitive attributes = (Race, Sex) where Race $\\in$ {Black, White} and Sex $\\in$ {Male, Female}. In Enum encoding, we compute metrics for all four combinations: Black-Male, Black-Female, White-Black, and White-Female. \n",
    "\n",
    "In the Learn encoding, the (SSAT) solver finds the most favored (least discriminated) and least favored (most discriminated) group efficeintly without us enumerating explicitly.\n",
    "\n",
    "\n",
    "First we show Enum encoding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact: 1\n",
      "Statistical Parity: 0.0\n",
      "Time taken 1.8231730461120605 seconds\n"
     ]
    }
   ],
   "source": [
    "metric_enum = Metric(model=clf, data=X_test, sensitive_attributes=dataset.known_sensitive_attributes, verbose=False, encoding=\"Enum\")\n",
    "metric_enum.compute()\n",
    "# details\n",
    "# print(metric_enum)\n",
    "# print()\n",
    "print(\"Disparate Impact:\", metric_enum.disparate_impact_ratio)\n",
    "print(\"Statistical Parity:\", metric_enum.statistical_parity_difference)\n",
    "print(\"Time taken\", metric_enum.time_taken, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show Learn encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact: 1.000000926244106\n",
      "Statistical Parity: -1.9999999961717307e-08\n",
      "Most favored group: {'race_Amer-Indian-Eskimo': ('==', 1), 'age_0': ('==', 1)}\n",
      "Least favored group: {'race_Amer-Indian-Eskimo': ('==', 1), 'age_0': ('==', 1)}\n",
      "Time taken 0.20220017433166504 seconds\n"
     ]
    }
   ],
   "source": [
    "metric_learn = Metric(model=clf, data=X_test, sensitive_attributes=dataset.known_sensitive_attributes, verbose=False, encoding=\"Learn\")\n",
    "metric_learn.compute()\n",
    "print(\"Disparate Impact:\", metric_learn.disparate_impact_ratio)\n",
    "print(\"Statistical Parity:\", metric_learn.statistical_parity_difference)\n",
    "print(\"Most favored group:\", metric_learn.most_favored_group)\n",
    "print(\"Least favored group:\", metric_learn.least_favored_group)\n",
    "print(\"Time taken\", metric_learn.time_taken, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both encodings have same output, the difference is in the efficiency of computing these metrics. Experimentally, the Learn encoding is superior than the Enum encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considering the correlation between sensitive and non-sensitive attributes\n",
    "\n",
    "In the previous two encodings (Learn and Enum), we consider independence of all attributes, which is unlikely in practical ML problems. Therefore, we propose Enum-correlation encoding that considers the conditional dependency of non-sensitive attributes on each compound group.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without correlation->\n",
      "Disparate Impact: 1\n",
      "Statistical Parity: 0.0\n",
      "Most favored group: {'sex': ('==', 1)}\n",
      "Least favored group: {'sex': ('==', 1)}\n",
      "\n",
      "\n",
      "\n",
      "With correlation->\n",
      "Disparate Impact: 0.36338477803088115\n",
      "Statistical Parity: 0.017751990000000002\n",
      "Most favored group: {'sex': ('!=', 1)}\n",
      "Least favored group: {'sex': ('==', 1)}\n",
      "\n",
      "\n",
      "\n",
      "Recalling the classification rule in CNF\n",
      "capital-gain_1 OR capital-gain_3 OR capital-loss_1 OR capital-loss_2 AND\n",
      "education-num_3 OR hours-per-week_2 OR not capital-gain_0\n",
      "\n",
      "Sensitive attribute in classifier? False\n"
     ]
    }
   ],
   "source": [
    "metric_enum = Metric(model=clf, data=X_test, sensitive_attributes=['sex'], verbose=False, encoding=\"Enum\").compute()\n",
    "print(\"Without correlation->\")\n",
    "print(\"Disparate Impact:\", metric_enum.disparate_impact_ratio)\n",
    "print(\"Statistical Parity:\", metric_enum.statistical_parity_difference)\n",
    "print(\"Most favored group:\", metric_enum.most_favored_group)\n",
    "print(\"Least favored group:\", metric_enum.least_favored_group)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"With correlation->\")\n",
    "metric_enum_cor = Metric(model=clf, data=X_test, sensitive_attributes=['sex'], verbose=False, encoding=\"Enum-correlation\").compute()\n",
    "print(\"Disparate Impact:\", metric_enum_cor.disparate_impact_ratio)\n",
    "print(\"Statistical Parity:\", metric_enum_cor.statistical_parity_difference)\n",
    "print(\"Most favored group:\", metric_enum_cor.most_favored_group)\n",
    "print(\"Least favored group:\", metric_enum_cor.least_favored_group)\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Recalling the classification rule in CNF\")\n",
    "rule = clf.get_rule(X_train.columns.to_list())\n",
    "print(rule)\n",
    "print(\"\\nSensitive attribute in classifier?\", \"sex\" in rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we see that without considering correlation among attributes, no discremination is detected. For example, the most favored group and the least favored group are identical (i.e., disparate impact = 1 and statistical parity = 0). However, when we consider correlation, there is a clear discremination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "\\[1\\] Ghosh B, Basu D, Meel KS. Justicia: A Stochastic SAT Approach to Formally Verify Fairness.\n",
    "\n",
    "\\[2\\] Ghosh B, Meel KS. IMLI: An incremental framework for MaxSAT-based learning of interpretable classification rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aff7118c20e07ed9fb612d208aa40ec32e694bdf8c4eda70182d9aa82faa566c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
